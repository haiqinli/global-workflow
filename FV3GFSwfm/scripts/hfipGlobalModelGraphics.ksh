#!/bin/ksh -l
#SBATCH -J HfipGlobalLargeScaleGFX
#SBATCH -D .
#SBATCH --partition=tjet,ujet,sjet
#SBATCH --ntasks=2
#SBATCH --mem=6G
#SBATCH --time=60
#SBATCH -q batch
#SBATCH -A gsd-fv3-dev

####### JKH notes #############
#
# 2019:  use /public/data/nhc/tcvitals/YYYYMMDDHH00.tcvitals for syndatDir
#
###############################

#########################################################################################################################
#                                                  INTRODUCTION                                                         #
#                                                                                                                       #
# This script uses GRIB1/GRIB2 files from HFIP global models to generate graphics for both large-scale and storm-scale  #
# environments using GrADS. It expects each data file to be processed to correspond to a unique forecast hour. All data #
# files are assumed to belong to the same large-scale domain. It requires 6 mandatory and 1 optional (last) arguments:  #
#                                                                                                                       #
# Arg 1) Simulation start date in format "YYYYMMDDHH", i.e., "2011082500".                                              #
# Arg 2) Model ATCF identifier (four letters, case-insensitive), i.e. "HWRF", "GFDL", "GFS", "AHW4", etc.               #
# Arg 3) Name of the organization who developed the model (space characters will be replaced with underscore),          #
#        i.e., "EMC/HRD", "GFDL", "Navy NRL" --> "Navy_NRL", etc...                                                     #
# Arg 4) Path to a non-empty text file pointing to the large-scale data files to be processed. Each line in this text   #
#        file should point to a data file that corresponds to a unique forecast hour. Lines in the file do not have to  #
#        be sorted by forecast hour. The information in each line is used to generate basin-wide (i.e. Atlantic and     #
#        East-Pac) diagnostic plots. Each line in the file is expected to have 3 comma-separated fields, as follows:    #
#           Field 1: Forecast hour, i.e., 0,6,12,...                                                                    #
#           Field 2: File format, "1" for GRIB1 and "2" for GRIB2.                                                      #
#           Field 3: Full path to the corresponding GRIB file.                                                          #
# Arg 5) Path (absolute or relative) to a temporary working directory where all processing can occur.                   #
#        This directory must have sufficient disk space for all the required processing to take place.                  #
# Arg 6) Path to the directory where the final graphics archive will be placed. The final graphics archive name         #
#        will contain a unique combination of the model ATCF identifier, simulation start time, and the processing time #
#        in format "YYYY-MM-DD-HHmmss-nnnnnnnnn" (which makes the file name unique). The archive will be compressed     #
#        with TAR-ZIP and  will have a ".tgz" extension, for example:                                                   #
#                           HFIP-Global-Graphics_FIM0_2011082200_2012-06-15-043215-597587000.tgz                        #
# Arg 7) (Optional) Path to an ATCF formatted text file holding the forecasted LAT/LON positions of existing storms at  #
#        various forecast hours. If this argument is missing, then only large-scale graphics will be generated using    #
#        the files listed in argument 4. Otherwise, storm-scale graphics will also be generated by zooming into the     #
#        regions of Atlantic and East-Pac storms found in the ATCF file. Storm scale graphics are plotted by matching   #
#        the forecast hour field in the file given by argument 4 to the forecast hour in the ATCF file. Those lines in  #
#        the ATCF file specified by argument 7 whose corresponding forecast hours are not also found in the text file   #
#        specified by argument 4 are simply ignored, as such implies that no large-scale data files are available for   #
#        those forecast hours. Only storms in the Atlantic and East-Pac basins are currently supported. Storms in other #
#        basins will be automatically ignored.                                                                          #
#                                                                                                                       #
# This script requires a working installation of GrADS with gribmap and wgrib (for GRIB1) and wgrib2 (for GRIB2). For   #
# questions or bugs, please email the script authors at: Thiago.Quirino@noaa.gov and David.A.Zelinsky@noaa.gov          #
#                                                                                                                       #
# Developer notes:                                                                                                      # 
# -Custom actions taken for FIM model: GRIB1 support only (for now): (1)Extracted pressure level variables in original  #
#                 GRIB1 file into a new working GRIB1 file instead of symlinking original GRIB1 file directory into     #
#                 working directory. (2) After invoking grib2ctl, corrected resulting definition of the MSLMAmsl        #
#                 variable in resulting control file. (3)Renamed variable PMSLmsl to MSLMAmsl in hfipGlobal*.gs scripts.#
#                                                                                                                       #
#########################################################################################################################


#########################################################################################################################
#                                                Set global variables                                                   #
#########################################################################################################################
# The directory where the required GrADS, Java, and Perl scripts needed to generate the graphics reside.
#JKHhfipPackegePath="/lfs4/HFIP/hfipprd/userScripts/HFIP-GRAPHICS"
hfipPackegePath="/lfs4/HFIP/gsd-fv3-hfip/rtruns/UFS-CAMsuite/FV3GFSwfm/scripts/HFIP-GRAPHICS"

# Add the required grads, wgrib, wgrib2, gribmap, ncdump, and ncks executables to the path
#ulimit -s unlimited
ulimit -s 1097152
module purge ; module load intel szip hdf5 mvapich2 netcdf grads nco wgrib2 cnvgrib 

# GrADS environmental variables
#export GRADS=/opt/grads/2.0.1/
#export GASCRP=/opt/grads/2.0.1/bin
#export GADDIR=/home/kyeh/GrADS/data
export GAUDFT=/home/fiorino/grads/udf/udft
export GADDIR=/mnt/lfs1/BMC/fim/fiorino/w21/app/grads/data
export GASCRP=/mnt/lfs1/BMC/fim/fiorino/w21/app/grads/gslib
export GA1UDXT=/mnt/lfs1/BMC/fim/fiorino/w21/app/opengrads1.10/bin/gex/udxt
export GA2UDXT=/mnt/lfs1/BMC/fim/fiorino/w21/app/grads-2.0.1.oga.1/Classic/bin/gex/udxt

# GrADS executable
grads=`which grads`

# The GrADS' "gribmap" tool needed to support GRIB1/GRIB2 files
gribmap=`which gribmap`

# The wgrib executable required to handle FIM GRIB1 data files
wgrib=`which wgrib`

# Path to syndat_tcvitals.YYYY files needed to retrieve storm names given storm ID and date from ATCF files
#syndatDir="/lfs1/projects/hwrf-vd/hwrf-input/SYNDAT"
#syndatDir="/public/data/nhc/syndat"
#JKHsyndatDir="/lfs4/HFIP/gsd-fv3-hfip/fv3-ccpp/syndat"
syndatDir="/public/data/nhc/tcvitals"

#########################################################################################################################
#                     Parse input arguments and derive any other required variables from them.                          #
#########################################################################################################################
#######################################################################
# Check the number of arguments passed to the script
if(test $# -lt 6) then
    echo "Error: Invalid number of arguments ($#). This script requires 6 mandatory and 1 optional arguments. See script header for details."
    exit 100
fi

#######################################################################
# Parse the simulation start time
dayAndCycle=`echo "$1" | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}' | awk '($1~"^[0-9][0-9]*$"){print}' | awk '(length($1)==10){print}'`
if(test -z "$dayAndCycle" ) then
    echo "Error: Invalid simulation start time ('$2'). Must be in format YYYYMMDDHH (i.e., 2011082500)."
    exit 4
fi

#######################################################################
# Parse the ATCF model identifier
modelAtcfId=`echo "$2" | tr -d ' ' | tr '[a-z]' '[A-Z]' | awk '(length($1)==4){print $1}'`
if(test -z "$modelAtcfId") then
    echo "Error: Invalid model ATCF ID ('$3'). Must be composed of 4 alpha-numeric characters."
    exit 5
fi

#######################################################################
# Parse the name of the organization
modelOrgName=`echo "$3" | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}' | tr ' ' '_'` #Replace spaces with "_" from the organization name

#######################################################################
# Parse the path to the text file pointing to large-scale data files
dataTextFile=`readlink -m "$4"` #Convert to full path
if(! test -s "$dataTextFile" || ! test -r "$dataTextFile") then
    echo "Error: Text file '$dataTextFile' is empty or unreadable."
    exit 6
fi

# Check the format of the comma-separated file. The format will be checked again while each GRIB file is being processed.
lineCounter=0   #Initialize the line counter
currDir=`pwd`   #Store the current directory
cd /var         #Enter a system owned directory to ensure the listed GRIB file paths are absolute paths (after invoking readlink)
while read line
do
    #Increment line counter
    lineCounter=`expr $lineCounter + 1`

    #Remove leading/trailing whitespaces from line
    line=`echo "$line" | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}'`

    #Skip blank lines
    if(test `echo "$line" | awk '{print length($1)}'` -le 0) then
        continue
    fi

    #Count the number of fields in the line
    numFields=`echo "$line" | awk -F "," '{print NF}'`
    if(test $numFields -ne 3) then
        echo "Error: Invalid number of fields in line $lineCounter of file '$dataTextFile', line contains $numFields fields but should contain 3 fields."
        exit 7
    fi

    #Extract the fields from the line.
    hour=`echo "$line" | awk -F "," '{print $1}' | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}' | awk '($1~"^[0-9][0-9]*$"){printf "%03d\n",$1*1.0}'`  #Ensure hour field is numeric and has leading 0s.
    fileFormat=`echo "$line" | awk -F "," '{print $2}' | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}' | awk '($1==1 || $1==2){print}'`
    file=`echo "$line" | awk -F "," '{print $3}' | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}'`
    file=`readlink -m "$file"`  #Convert to full path to ensure file exists

    #Ensure fields are valid
    if(test -z "$hour" || test -z "$fileFormat" || ! test -s "$file" || ! test -r "$file") then
        echo "Error: Invalid record in line $lineCounter of file '$dataTextFile'. This line corresponds to a large-scale data file. Field 1 must be a positive integer (forecast hour), field 2 must either 1 or 2 (GRIB1 or GRIB2), and field 3 must be a full path to a non-empty and readable GRIB file."
        exit 8
    fi
done < "$dataTextFile"

#######################################################################
# Parse the path to the temporary directory
cd "$currDir"   #Return to directory where the script was invoked to process any remaining relative paths
tempDir=`readlink -m "$5"`
workDir=`echo "$tempDir/HFIP-Graphics-TEMP_${USER}-${HOSTNAME}-$$"`  #Create a unique working directory
mkdir -p "$workDir"
if(! test -d "$workDir") then
    echo "Error: Could not create a temporary working directory inside '$tempDir'."
    exit 9
fi

#######################################################################
# Parse the path to the output directory
outputDir=`readlink -m "$6"`
touch "$outputDir/${USER}-${HOSTNAME}-$$" >/dev/null 2>&1   #Attempt to write in the output directory
if(! test -d "$outputDir" || ! test -f "$outputDir/${USER}-${HOSTNAME}-$$") then
    echo "Error: Cannot write to the specified archive destination directory '$outputDir'."
    rm -rf "$workDir"   #Delete the temporary working directory that was created in the last step
    exit 10
fi
rm -f "$outputDir/${USER}-${HOSTNAME}-$$" #Delete the "write-test" file created by the touch command.

#######################################################################
# Parse the path to the ATCF file pointing to forecasted storm information

#If the text file does not exist, point it to /dev/null
if(! test -f "$7") then
    stormTextFile="/dev/null"
else
    #Convert to full path
    atcfFile=`readlink -m "$7"`
    if(! test -s "$atcfFile" || ! test -r "$atcfFile") then
        echo "Error: ATCF file '$atcfFile' is empty or unreadable."
        rm -rf "$workDir"   #Delete the temporary working directory
        exit 6
    fi

    #Create the storm file, which will remain empty if no valid ATCF entries were found in the user's ATCF file
    stormTextFile="$workDir/atcfCustomFormat.txt"
    touch "$stormTextFile"

    #Let's convert the ATCF data to a custom format file in the working directory having with 4 comma-separated fields:
    #(1) forecast hour, (2) storm name, (3) lat (-90,90), and (4) lon (-180,180).
    #First, check if the required syndat file exists so that storm names can be retrieved
    YYYY=`echo "$dayAndCycle" | cut -c1-4`
    syndatDate=`echo "$dayAndCycle" | awk '{printf "%s %s00",substr($1,1,8),substr($1,9,2)}'`
#JKH    if(test -r "$syndatDir/syndat_tcvitals.${YYYY}") then
    if(test -r "$syndatDir/${dayAndCycle}00.tcvitals") then
        #The first step is to retain only the required fields of unique ATCF entries.
        grep " ${dayAndCycle}," "$atcfFile" | awk -F "," '{if(NF<8)next;basin=$1;gsub(/^[ \t]+|[ \t]+$/,"",basin);if(basin!="EP" && basin!="AL")next;if(basin=="AL")basin="L";else basin="E";stormcode=$2;gsub(/^[ \t]+|[ \t]+$/,"",stormcode);if(stormcode!~"^[0-9][0-9]*$")next;fhour=$6;gsub(/^[ \t]+|[ \t]+$/,"",fhour);if(fhour!~"^[0-9][0-9]*$")next;fhour=fhour*1.0;tempLat=$7;gsub(/^[ \t]+|[ \t]+$/,"",tempLat);if(length(tempLat)<2 || substr(tempLat,1,length(tempLat)-1)!~"^[0-9][0-9]*$" || (substr(tempLat,length(tempLat))!="N" && substr(tempLat,length(tempLat))!="S"))next;if(substr(tempLat,length(tempLat))=="N")lat=substr(tempLat,1,length(tempLat)-1)/10.0;else lat=substr(tempLat,1,length(tempLat)-1)/-10.0;tempLon=$8;gsub(/^[ \t]+|[ \t]+$/,"",tempLon);if(length(tempLon)<2 || substr(tempLon,1,length(tempLon)-1)!~"^[0-9][0-9]*$" || (substr(tempLon,length(tempLon))!="E" && substr(tempLon,length(tempLon))!="W"))next;if(substr(tempLon,length(tempLon))=="E")lon=substr(tempLon,1,length(tempLon)-1)/10.0;else lon=substr(tempLon,1,length(tempLon)-1)/-10.0;printf "%s%s,%s,%.1f,%.1f\n",stormcode,basin,fhour,lat,lon}' | sort -u > "$workDir/tempAtcf.txt"
        
        #In the next step, we process each unique line retained from the ATCF file and for each line's
        #storm code, basin, and day-and-cycle, we look for a corresponding unique storm name.
        while read line
        do
            #Parse the fields from the temporary file
            stormId=`echo "$line" | awk -F "," '{print $1}'`
            hour=`echo "$line" | awk -F "," '{print $2}'`
            lat=`echo "$line" | awk -F "," '{print $3}'`
            lon=`echo "$line" | awk -F "," '{print $4}'`
            
            #Look for a unique storm name in the syndat file
#JKH            stormName=`grep " $syndatDate " "$syndatDir/syndat_tcvitals.${YYYY}" | grep " $stormId " | awk 'END{print $3}'`
            stormName=`grep " $syndatDate " "$syndatDir/${dayAndCycle}00.tcvitals" | grep " $stormId " | awk 'END{print $3}'`
            
            #If a storm name was not found, skip to next line
            if(test -z "$stormName") then
                continue
            fi
            
            #Store the required fields in a line to be processed later
            echo "$hour,$stormName$stormId,$lat,$lon" >> "$stormTextFile"
        done < "$workDir/tempAtcf.txt"
    fi
fi

############################################################################################################################################################
#  The ATCF file format does not need to be checked here because it can change as a forecast runs. Only valid and complete lines will be processed anyway. #
############################################################################################################################################################
## Check the format of the ATCF file. The format will be checked again while each GRIB file is being processed.
#lineCounter=0   #Initialize the line counter
#while read line
#do
#    #Increment line counter
#    lineCounter=`expr $lineCounter + 1`
#
#    #Remove leading/trailing whitespaces from line
#    line=`echo "$line" | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}'`
#
#    #Skip blank lines
#    if(test `echo "$line" | awk '{print length($1)}'` -le 0) then
#        continue
#    fi
#
#    #Count the number of fields in the line
#    numFields=`echo "$line" | awk -F "," '{print NF}'`
#    if(test $numFields -ne 4) then
#        echo "Error: Invalid number of fields in line $lineCounter of file '$stormTextFile', line contains $numFields fields but should contain 4 fields."
#        rm -rf "$workDir"   #Delete the temporary working directory
#        exit 7
#    fi
#
#    #Extract the fields from the line.
#    hour=`echo "$line" | awk -F "," '{print $1}' | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}' | awk '($1~"^[0-9][0-9]*$"){printf "%03d\n",$1*1.0}'`  #Ensure hour field is numeric and has leading 0s.
#    storm=`echo "$line" | awk -F "," '{print $2}' | tr -d ' ' | tr '[a-z]' '[A-Z]' | awk '(length($1)>=4 && (substr($1,length($1))=="L" || substr($1,length($1))=="E") && substr($1,length($1)-2,2)~"^[0-9][0-9]*$"){print}'`  #Uppercase the storm name, remove spaces, and ensure it is in the format <stormname><cc><b>, i.e., "IRENE09L".
#    lat=`echo "$line" | awk -F "," '{print $3}' | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}' | awk '{if((substr($1,1,1)=="-" && substr($1,2)~"^[0-9]+([.][0-9]+)?$") || $1~"^[0-9]+([.][0-9]+)?$")print}' | awk '($1>=-90.0 && $1<=90.0){print}'`
#    lon=`echo "$line" | awk -F "," '{print $4}' | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}' | awk '{if((substr($1,1,1)=="-" && substr($1,2)~"^[0-9]+([.][0-9]+)?$") || $1~"^[0-9]+([.][0-9]+)?$")print}' | awk '($1>=-180.0 && $1<=180.0){print}'`
#
#    #Ensure fields are valid
#    if(test -z "$hour" || test -z "$storm" || test -z "$lat" || test -z "$lon") then
#        echo "Error: Invalid record in line $lineCounter of file '$stormTextFile'. This line corresponds to a storm forecast information. Field 1 must be a positive integer (forecast hour), field 2 must be a storm name with cyclone ID and basin identifier (i.e. 'IRENE09L'), field 3 must be a real number ranging from -90.0 to 90.0 (storm latitude), and field 4 must be a real number ranging from -180.0 to 180.0 (storm longitude)."
#        rm -rf "$workDir"   #Delete the temporary working directory
#        exit 8
#    fi
#done < "$stormTextFile"

#######################################################################
# Display the arguments and derived variables to the user
echo "-HFIP Stream 1.5 Global Model Graphics Package started on `date` by user '$USER' on host '$HOSTNAME'."
echo "-Input arguments ($#) and derived variables:"
echo "   -Simulation start time: '$dayAndCycle'"
echo "   -Model ATCF ID:         '$modelAtcfId'"
echo "   -Organization name:     '$modelOrgName'"
echo "   -Data file list:        '$dataTextFile'"
if(test -s "$stormTextFile") then
    echo "   -ATCF file:             '$stormTextFile'"
else
    if(test "$stormTextFile" = "/dev/null") then
        echo "   -ATCF file:             (No file specified)"
    else
        echo "   -ATCF file:             (No valid entries found)"
    fi
fi
echo "   -Working directory:     '$workDir'"
echo "   -Final directory:       '$outputDir'"
echo "   -HFIP Graphics Package: '$hfipPackegePath'"


#########################################################################################################################
#                                       Process each GRIB file to generate images                                       #
#########################################################################################################################
#######################################################################
# First, let's copy the required GrADS, Java, executables, and Perl scripts to the working directory, then enter it.
echo "-Copying GrADS scripts to working directory."
cp "$hfipPackegePath"/{GRADS,JAVA,PERL}/* "$workDir"

#Now enter the working directory where all graphical processing will occur.
cd "$workDir"
mkdir images    #Create the "images/" directory where generated images will be placed...
mkdir WEBSITE   #...and also create a directory to represent the HFIP website root directory.
mkdir TEMP      #This is a general purpose temporary processing directory

#######################################################################
# Next, let's loop through each GRIB file listed by the user and process them sequentially.
lineCounter=0 #Initialize the line counter (used in error messages)
echo "-Processing GRIB files in '$dataTextFile'."
while read line
do
    #Increment the line counter
    lineCounter=`expr $lineCounter + 1`

    #Remove leading/trailing whitespaces from line
    line=`echo "$line" | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}'`

    #Skip blank lines
    if(test `echo "$line" | awk '{print length($1)}'` -le 0) then
        continue
    fi

    #Inform user which line entry in the file list is being processed
    echo "   -Processing data file in line $lineCounter. "

    #Count the number of fields in the line
    numFields=`echo "$line" | awk -F "," '{print NF}'`
    if(test $numFields -ne 3) then
        echo "      -Warning: Invalid number of fields in line $lineCounter of file '$dataTextFile', line contains $numFields fields but should contain 3 fields."
        continue
    fi

    #Extract the fields from the line.
    hour=`echo "$line" | awk -F "," '{print $1}' | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}' | awk '($1~"^[0-9][0-9]*$"){printf "%03d\n",$1*1.0}'`  #Ensure hour field is numeric and has leading 0s.
    fileFormat=`echo "$line" | awk -F "," '{print $2}' | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}' | awk '($1==1 || $1==2){print}'`
    file=`echo "$line" | awk -F "," '{print $3}' | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}'`
    file=`readlink -m "$file"`  #Convert to full path to ensure file exists
    suffix="p"  #Used in the name of the generated GrADS control file

    #Ensure fields are valid
    if(test -z "$hour" || test -z "$fileFormat" || ! test -s "$file" || ! test -r "$file") then
        echo "      -Warning: Invalid record in line $lineCounter of file '$dataTextFile'. This line corresponds to a large-scale data file. Field 1 must be a positive integer (forecast hour), field 2 must either 1 or 2 (GRIB1 or GRIB2), and field 3 must be a full path to a non-empty and readable GRIB file."
        continue
    fi

    #Symlink the GRIB file into the working directory. For FIM model, assume GRIB1 and extract pressure-level variables using wgrib into a new GRIB1 file.
    fileName=`basename "$file"` #The file name only, without the path
    if(! test -z `echo "$modelAtcfId" | awk '($1~"FIM"){print}'`) then  #Keyword "FIM" in ATCF ID, for now, denotes FIM configurations.
        echo "      -Extracting pressure-level variables into new GRIB file in working directory."
        $wgrib "$file" 2>/dev/null | grep -v ":hybrid lev " | wgrib -s "$file" -i -grib -o "$workDir/$fileName" >/dev/null 2>&1 #Fails automatically for GRIB2.
        if(test $? -ne 0 || ! test -s "$workDir/$fileName") then
            echo "         -Warning: Extraction with wgrib failed."
            continue
        fi
    else
        echo "      -Symlinking '$fileName' to working directory."
        ln -sf "$file" "$workDir"
        if(! test -f "$workDir/$fileName") then
            echo "         -Warning: Could not symlink file."
            continue
        fi
    fi

    #Use grib2ctl.pl (GRIB1) or g2ctl.pl (GRIB2) to create a control file
    if(test $fileFormat -eq 1) then
        echo "      -Creating GrADS control file for GRIB${fileFormat} using grib2ctl."
        ./grib2ctl.pl "$workDir/$fileName" > "$workDir/${hour}_${suffix}.ctl" 2>/dev/null
        if(test $? -ne 0 || ! test -s "$workDir/${hour}_${suffix}.ctl") then
            echo "         -Warning: Program grib2ctl failed."
            continue
        fi

        #Add Mike Fiorino's fix to the MSLMAmsl variable in GrADS control files generated from FIM's GRIB1 files
        if(! test -z `echo "$modelAtcfId" | awk '($1~"FIM"){print}'`) then  #Keyword "FIM" in ATCF ID, for now, denotes FIM configurations.
            sed -i "s:MSLMAmsl  0 129,102,0:MSLMAmsl  0 129,102,1:g" "$workDir/${hour}_${suffix}.ctl"
        fi
    else
        echo "      -Creating GrADS control file for GRIB${fileFormat} using g2ctl."
        ./g2ctl.pl "$workDir/$fileName" > "$workDir/${hour}_${suffix}.ctl" 2>/dev/null
        if(test $? -ne 0 || ! test -s "$workDir/${hour}_${suffix}.ctl") then
            echo "         -Warning: Program g2ctl failed."
            continue
        fi
    fi

    #Generate the GRIB index file needed by GrADS to read the GRIB data file
    echo "      -Creating a GRIB${fileFormat} index file using gribmap."
    $gribmap -i "$workDir/${hour}_${suffix}.ctl" -0 >/dev/null 2>&1  #This requires a working installation of GrADS with gribmap
    if(! test -s "$workDir/${fileName}.idx") then
        echo "         -Warning: Program gribmap failed."
        continue
    fi

    # Change the variable names in Dave's GrADS scripts according to the output of grib2ctl/g2ctl for the various models.
    # The following models all have the same variable names in their control files: HWRF, GFDL, GFS-EnKF, and AHW.
    # FIM and FV3 control files requires special treatment.
    # AHW and UWisc data files are in netCDF format.
    #JKH if(! test -z `echo "$modelAtcfId" | awk '($1~"FIM"){print}'`) then  #Keyword "FIM" in ATCF ID, for now, denotes FIM configurations.
    if(! test -z `echo "$modelAtcfId" | awk '($1~"UCS"){print}' | awk '($1~"FIM"){print}' | awk '($1~"FV3G"){print}'`) then  
        #Replace the keyword "PRMSLmsl" with "MSLMAmsl" in the GrADS scripts
        echo "      -Renaming 'PRMSLmsl' to 'MSLMAmsl' in GrADS scripts for $modelAtcfId."
        sed -i "s:PRMSLmsl:MSLMAmsl:g" hfipGlobal*.gs
    fi

    #Run GrADS. Images will be placed inside the images directory
    echo "      -Running GrADS to generate large-scale images."
    echo "         -Command: grads -blc \"run hfipGlobalBasinGraphics.gs $hour 1 $dayAndCycle $modelAtcfId $modelOrgName\""
    #echo "######################################## START GRADS OUTPUT ########################################"
    rm -f images/* #Empty the directory
    $grads -blc "run hfipGlobalBasinGraphics.gs $hour 1 $dayAndCycle $modelAtcfId $modelOrgName" >/dev/null 2>&1   #Supress GrADS output
    #echo "######################################### END GRADS OUTPUT #########################################"

    #Check if images were created
    if(test `ls images/*.png 2>/dev/null | wc -l` -le 0) then
        echo "         -Warning: GrADS did not generate any images."
        continue
    fi

    #Rename the images generated by GrADS as required by the HFIP website.
    rename "_p.f" "_p_f" images/*.png   #This line will rename images from both Atlantic and East-Pac.

    #Create the directory structure inside the WEBSITE directory that is required to adequately categorize the generated Atlantic basin images in the HFIP website.
    if(test `ls images/atl.* 2>/dev/null | wc -l` -gt 0) then
        echo "      -Processing Atlantic images."
        theModel=`echo $modelAtcfId | tr '[A-Z]' '[a-z]'`
        theDate=$dayAndCycle
        theDomain="Atlantic"
        webDir=`echo "$workDir/WEBSITE/${theModel}/${theDate}/${theDomain}"`
        echo "         -Creating local web directory structure '${theModel}/${theDate}/${theDomain}/'."
        mkdir -p "$webDir"
        if(! test -d "$webDir") then
            echo "            -Warning: Failed to create '$webDir'."
            continue
        fi

        #Move the Atlantic images to the TEMP directory where they will be renamed first
        rm -f TEMP/* ; mv images/atl.* TEMP/

        #Rename the Atlantic images in the TEMP directory as required by the HFIP website.
        rename "atl.${dayAndCycle}.${modelAtcfId}." "" TEMP/atl.*

        #Move the renamed Atlantic basin images to the directory structure created for the HFIP website.
        echo "         -Moving Atlantic images to local web directory structure."
        mv TEMP/*.png "$webDir" 2>/dev/null
        if(test $? -ne 0) then
            echo "            -Warning: Failed to move images."
            continue
        fi
    fi

    #Create the directory structure inside the WEBSITE directory that is required to adequately categorize the generated East-Pac basin images in the HFIP website.
    if(test `ls images/epac.* 2>/dev/null | wc -l` -gt 0) then
        echo "      -Processing East-Pac images."
        theModel=`echo $modelAtcfId | tr '[A-Z]' '[a-z]'`
        theDate=$dayAndCycle
        theDomain="East_Pacific"
        webDir=`echo "$workDir/WEBSITE/${theModel}/${theDate}/${theDomain}"`
        echo "         -Creating local web directory structure '${theModel}/${theDate}/${theDomain}/'."
        mkdir -p "$webDir"
        if(! test -d "$webDir") then
            echo "            -Warning: Failed to create '$webDir'."
            continue
        fi

        #Move the East-Pac images to the TEMP directory where they will be renamed first
        rm -f TEMP/* ; mv images/epac.* TEMP/

        #Rename the East-Pac images in the TEMP directory as required by the HFIP website.
        rename "epac.${dayAndCycle}.${modelAtcfId}." "" TEMP/epac.*

        #Move the renamed East-Pac basin images to the directory structure created for the HFIP website.
        echo "         -Moving East-Pac images to local web directory structure."
        mv TEMP/*.png "$webDir" 2>/dev/null
        if(test $? -ne 0) then
            echo "            -Warning: Failed to move images."
            continue
        fi
    fi

    #Now let's process storm-scale graphics for all storms forecasted during the specified forecast hour.
    #Let's loop through the ATCF text file and look for valid entries whose forecast hours match the value of "$hour".
    stormLineCounter=0  #Initialize the line counter
    countStormsProcessed=-1   #Initialize the counter for storms processed. Initial value indicates no ATCF file was given.
    while read stormLine
    do
        #Inform user before processing first line and reset the storms processed counter
        if(test $stormLineCounter -le 0) then
            echo "      -Processing storm-scale graphics for forecast hour $hour."
            countStormsProcessed=0    #Indicates a storm file is found
        fi

        #Increment line counter
        stormLineCounter=`expr $stormLineCounter + 1`

        #Remove leading/trailing whitespaces from line
        stormLine=`echo "$stormLine" | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}'`

        #Skip blank lines
        if(test `echo "$stormLine" | awk '{print length($1)}'` -le 0) then
            continue
        fi

        #Count the number of fields in the line
        numFields=`echo "$stormLine" | awk -F "," '{print NF}'`
        if(test $numFields -ne 4) then
            echo "         -Warning: Invalid number of fields in line $stormLineCounter of file '$stormTextFile', line contains $numFields fields but should contain 4 fields."
            continue
        fi

        #Extract the fields from the line.
        stormHour=`echo "$stormLine" | awk -F "," '{print $1}' | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}' | awk '($1~"^[0-9][0-9]*$"){printf "%03d\n",$1*1.0}'`  #Ensure hour field is numeric and has leading 0s.
        storm=`echo "$stormLine" | awk -F "," '{print $2}' | tr -d ' ' | tr '[a-z]' '[A-Z]' | awk '(length($1)>=4 && (substr($1,length($1))=="L" || substr($1,length($1))=="E") && substr($1,length($1)-2,2)~"^[0-9][0-9]*$"){print}'`  #Uppercase the storm name, remove spaces, ensure it is in the format <stormname><cc><b>, and only AL and EP are supported, i.e., "IRENE09L".
        lat=`echo "$stormLine" | awk -F "," '{print $3}' | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}' | awk '{if((substr($1,1,1)=="-" && substr($1,2)~"^[0-9]+([.][0-9]+)?$") || $1~"^[0-9]+([.][0-9]+)?$")print}' | awk '($1>=-90.0 && $1<=90.0){print $1*10.0}'` #Degrees north is positive
        lon=`echo "$stormLine" | awk -F "," '{print $4}' | awk '{gsub(/^[ \t]+|[ \t]+$/, "");print}' | awk '{if((substr($1,1,1)=="-" && substr($1,2)~"^[0-9]+([.][0-9]+)?$") || $1~"^[0-9]+([.][0-9]+)?$")print}' | awk '($1>=-180.0 && $1<=180.0){print $1*-10.0}'` #Degrees west is positive

        #Ensure fields are valid
        if(test -z "$stormHour" || test -z "$storm" || test -z "$lat" || test -z "$lon") then
            echo "         -Warning: Invalid record in line $stormLineCounter of file '$stormTextFile'. This line corresponds to a storm forecast information. Field 1 must be a positive integer (forecast hour), field 2 must be a storm name with cyclone ID and basin identifier (i.e. 'IRENE09L'), field 3 must be a real number ranging from -90.0 to 90.0 (storm latitude), and field 4 must be a real number ranging from -180.0 to 180.0 (storm longitude)."
            continue
        fi

        #Check if the forecast hours match
        if(test $stormHour -ne $hour) then
            continue    #Simply move on to the next line in the file
        fi

        #At this stage, the forecast hours match and all fields are valid, so let's run GrADS and generate storm-scale graphics.
        echo "         -Running GrADS to generate storm-scale images for '$storm'."
        echo "            -Command: grads -blc \"run hfipGlobalNestGraphics.gs $hour 1 $storm $dayAndCycle $modelAtcfId $modelOrgName $lat $lon\""
        #echo "######################################## START GRADS OUTPUT ########################################"
        rm -f images/* #Empty the directory
        $grads -blc "run hfipGlobalNestGraphics.gs $hour 1 $storm $dayAndCycle $modelAtcfId $modelOrgName $lat $lon" >/dev/null 2>&1   #Supress GrADS output
        #echo "######################################### END GRADS OUTPUT #########################################"

        #Check if images were created
        if(test `ls images/*.png 2>/dev/null | wc -l` -le 0) then
            echo "            -Warning: GrADS did not generate any images."
            continue
        fi

        #Rename the images generated by GrADS as required by the HFIP website.
        rename "${storm}.${dayAndCycle}.${modelAtcfId}." "" images/*.png;
        rename "_n.f" "_n_f" images/*.png

        #Create the directory structure inside the WEBSITE directory that is required to adequately categorize the generated images in the HFIP website.
        theModel=`echo $modelAtcfId | tr '[A-Z]' '[a-z]'`
        theDate=$dayAndCycle
        stormName=`echo $storm | awk '{print substr($1,1,length($1)-3)}'`   #i.e., IRENE
        stormId=`echo $storm | rev | cut -c2-3| rev`
        stormBasin=`echo $storm | rev | cut -c1-1 | awk '{if($1=="L") print "AL"; else if($1=="E") print "EP"; else print ""}'`
        tempName=`echo $stormName | tr '[A-Z]' '[a-z]' | sed -e "s/\b\(.\)/\u\1/g"` #Storm name with first letter capitalized only
        theDomain=`echo "${tempName}_${stormId}${stormBasin}"`
        webDir=`echo "$workDir/WEBSITE/${theModel}/${theDate}/${theDomain}"`
        echo "         -Creating local web directory structure '${theModel}/${theDate}/${theDomain}/'."
        mkdir -p "$webDir"
        if(! test -d "$webDir") then
            echo "            -Warning: Failed to create '$webDir'."
            continue
        fi

        #Move all images generated by GrADS to the directory structure created for the HFIP website.
        echo "         -Moving images to local web directory structure."
        mv images/*.png "$webDir" 2>/dev/null
        if(test $? -ne 0) then
            echo "            -Warning: Failed to move images."
            continue
        fi

        #Increment the counter indicating how many storms were processed
        countStormsProcessed=`expr $countStormsProcessed + 1`
    done < "$stormTextFile"

    #Inform user how many storms were processed. Warn user if all entries in file are invalid.
    if(test $countStormsProcessed -ge 0) then
        if(test $countStormsProcessed -eq 0); then warnText="(processing errors ocurred or ATCF file is improperly formatted)"; fi
        echo "         -Total storms processed: $countStormsProcessed $warnText"
    fi
done < "$dataTextFile"

#######################################################################
#Now, check if any images were generated by searching inside the "WEBSITE" directory located inside the working directory.
if(test `find "$workDir/WEBSITE/" -type f 2>/dev/null | grep ".png" | wc -l` -le 0) then
    echo "Error: No images were generated."
    rm -rf "$workDir"   #Delete the temporary working directory
    exit 11
fi

#######################################################################
# Now, create an archive of the images inside the working directory
cd "$workDir/WEBSITE"                                                                                   #Enter the "images/" directory
currentTime=`date "+%Y-%m-%d-%H%M%S-%N"`                                                                #Get the current time in YYYYMMDDHHmmss format
archivePath=`echo "$workDir/HFIP-Global-Graphics_${modelAtcfId}_${dayAndCycle}_${currentTime}.tgz"`     #Create a unique name for the archive
echo "-Creating TAR-ZIP archive: HFIP-Global-Graphics_${modelAtcfId}_${dayAndCycle}_${currentTime}.tgz" #Inform the user
tar -cvzf "$archivePath" * >/dev/null 2>&1                                                              #Create the TAR-ZIP archive with all images. Suppress output.
if(test $? -ne 0) then
    echo "Error: TAR-ZIP failed to generate an archive of images."
    rm -rf "$workDir"   #Delete the temporary working directory
    exit 12
fi

#######################################################################
# Next, move the graphics archive to the destination directory
echo "-Moving TAR-ZIP archive to destination directory."
mv "$archivePath" "$outputDir"
if(test $? -ne 0) then
    echo "Error: Could not move graphics archive to destination directory."
    rm -rf "$workDir"   #Delete the temporary working directory exit 13
fi

#######################################################################
#Finally, echo a success notification to the user and quit.
echo "-HFIP Stream 1.5 Global Model Graphics Package ended on `date`. Good bye!"
rm -rf "$workDir"   #Delete the temporary working directory
